"""Intelligent quality scoring utilities."""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional

from .ast_parser import CodeMetrics, QualityIssue
from .static_analyzers import StaticAnalysisResult

logger = logging.getLogger(__name__)

class QualityCategory(Enum):
    """è´¨é‡åˆ†ç±»"""
    MAINTAINABILITY = "maintainability"
    RELIABILITY = "reliability" 
    SECURITY = "security"
    PERFORMANCE = "performance"
    STYLE = "style"
    COMPLEXITY = "complexity"

@dataclass
class QualityScore:
    """è´¨é‡è¯„åˆ†è¯¦æƒ…"""
    total_score: float  # æ€»åˆ† 0-100
    category_scores: Dict[QualityCategory, float] = field(default_factory=dict)
    grade: str = ""  # A+, A, B, C, D, F
    technical_debt_hours: float = 0.0
    priority_issues: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    strengths: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """è®¡ç®—ç­‰çº§"""
        if self.total_score >= 95:
            self.grade = "A+"
        elif self.total_score >= 90:
            self.grade = "A"
        elif self.total_score >= 80:
            self.grade = "B"
        elif self.total_score >= 70:
            self.grade = "C"
        elif self.total_score >= 60:
            self.grade = "D"
        else:
            self.grade = "F"

@dataclass
class ScoringWeights:
    """è¯„åˆ†æƒé‡é…ç½®"""
    # ä¸»è¦ç»´åº¦æƒé‡ (æ€»å’Œä¸º1.0)
    maintainability: float = 0.25
    reliability: float = 0.25
    security: float = 0.20
    performance: float = 0.15
    style: float = 0.10
    complexity: float = 0.05
    
    # å­æŒ‡æ ‡æƒé‡
    cyclomatic_complexity_weight: float = 0.4
    cognitive_complexity_weight: float = 0.3
    nesting_depth_weight: float = 0.3
    
    # æƒ©ç½šæƒé‡
    error_penalty: float = 20.0
    warning_penalty: float = 5.0
    info_penalty: float = 1.0
    
    # å¥–åŠ±æƒé‡  
    good_practices_bonus: float = 5.0
    comprehensive_tests_bonus: float = 10.0

class IntelligentQualityScorer:
    """æ™ºèƒ½è´¨é‡è¯„åˆ†å™¨"""
    
    def __init__(self, weights: Optional[ScoringWeights] = None):
        self.weights = weights or ScoringWeights()
        
        # è´¨é‡é˜ˆå€¼é…ç½®
        self.thresholds = {
            'cyclomatic_complexity': {'good': 5, 'acceptable': 10, 'bad': 15},
            'cognitive_complexity': {'good': 10, 'acceptable': 20, 'bad': 30},
            'nesting_depth': {'good': 3, 'acceptable': 4, 'bad': 6},
            'function_length': {'good': 20, 'acceptable': 50, 'bad': 100},
            'maintainability_index': {'good': 80, 'acceptable': 60, 'bad': 40},
            'technical_debt_minutes': {'good': 30, 'acceptable': 120, 'bad': 480}
        }
        
        # æœ€ä½³å®è·µæ¨¡å¼
        self.good_practices = {
            'typescript': ['strict type checking', 'interfaces', 'generics'],
            'python': ['type hints', 'docstrings', 'list comprehensions'],
            'javascript': ['const usage', 'arrow functions', 'destructuring']
        }
    
    def calculate_quality_score(
        self, 
        metrics: CodeMetrics, 
        static_results: List[StaticAnalysisResult],
        quality_issues: List[QualityIssue]
    ) -> QualityScore:
        """è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ†"""
        
        # åˆå§‹åŒ–åˆ†ç±»è¯„åˆ†
        category_scores = {}
        
        # 1. ç»´æŠ¤æ€§è¯„åˆ†
        category_scores[QualityCategory.MAINTAINABILITY] = self._calculate_maintainability_score(
            metrics, static_results
        )
        
        # 2. å¯é æ€§è¯„åˆ†
        category_scores[QualityCategory.RELIABILITY] = self._calculate_reliability_score(
            metrics, static_results, quality_issues
        )
        
        # 3. å®‰å…¨æ€§è¯„åˆ†
        category_scores[QualityCategory.SECURITY] = self._calculate_security_score(
            static_results
        )
        
        # 4. æ€§èƒ½è¯„åˆ†
        category_scores[QualityCategory.PERFORMANCE] = self._calculate_performance_score(
            metrics, static_results
        )
        
        # 5. é£æ ¼è¯„åˆ†
        category_scores[QualityCategory.STYLE] = self._calculate_style_score(
            static_results
        )
        
        # 6. å¤æ‚åº¦è¯„åˆ†
        category_scores[QualityCategory.COMPLEXITY] = self._calculate_complexity_score(
            metrics
        )
        
        # è®¡ç®—åŠ æƒæ€»åˆ†
        total_score = self._calculate_weighted_total(category_scores)
        
        # åº”ç”¨å¥–åŠ±å’Œæƒ©ç½š
        total_score = self._apply_bonuses_and_penalties(
            total_score, metrics, static_results
        )
        
        # ç”Ÿæˆè¯„åˆ†è¯¦æƒ…
        quality_score = QualityScore(
            total_score=max(0, min(100, total_score)),
            category_scores=category_scores,
            technical_debt_hours=metrics.technical_debt_minutes / 60.0,
            priority_issues=self._identify_priority_issues(quality_issues, static_results),
            recommendations=self._generate_recommendations(metrics, static_results),
            strengths=self._identify_strengths(metrics, category_scores)
        )
        
        return quality_score
    
    def _calculate_maintainability_score(
        self, 
        metrics: CodeMetrics, 
        static_results: List[StaticAnalysisResult]
    ) -> float:
        """è®¡ç®—ç»´æŠ¤æ€§è¯„åˆ†"""
        mi_score = (
            float(metrics.maintainability_index)
            if hasattr(metrics, "maintainability_index")
            else float(self._estimate_maintainability_index(metrics))
        )
        base_score = max(0.0, min(100.0, mi_score))
        
        # å¤æ‚åº¦æƒ©ç½š
        complexity_penalty = 0
        if metrics.cyclomatic_complexity > self.thresholds['cyclomatic_complexity']['acceptable']:
            complexity_penalty += (metrics.cyclomatic_complexity - 10) * 2
        
        if metrics.cognitive_complexity > self.thresholds['cognitive_complexity']['acceptable']:
            complexity_penalty += (metrics.cognitive_complexity - 20) * 1.5
        
        # é•¿å‡½æ•°æƒ©ç½š
        long_function_penalty = len(metrics.long_functions) * 5
        
        # é‡å¤ä»£ç æƒ©ç½š
        duplicate_penalty = len(metrics.duplicated_code_blocks) * 8
        
        # é™æ€åˆ†æé—®é¢˜æƒ©ç½š
        static_penalty = sum(
            3 for result in static_results 
            if result.category == 'style' and result.severity == 'warning'
        )
        
        final_score = (
            base_score
            - complexity_penalty
            - long_function_penalty
            - duplicate_penalty
            - static_penalty
        )
        return max(0, min(100, final_score))
    
    def _calculate_reliability_score(
        self, 
        metrics: CodeMetrics,
        static_results: List[StaticAnalysisResult],
        quality_issues: List[QualityIssue]
    ) -> float:
        """è®¡ç®—å¯é æ€§è¯„åˆ†"""
        base_score = 100.0
        
        # é”™è¯¯ä¸¥é‡ç¨‹åº¦æƒ©ç½š
        error_penalty = sum(
            self.weights.error_penalty for result in static_results 
            if result.severity == 'error'
        )
        
        warning_penalty = sum(
            self.weights.warning_penalty for result in static_results 
            if result.severity == 'warning'
        )
        
        # è´¨é‡é—®é¢˜æƒ©ç½š
        quality_penalty = sum(
            15 if issue.severity == 'error' else 
            8 if issue.severity == 'warning' else 3
            for issue in quality_issues
        )
        
        # å¤æ‚åº¦å¯é æ€§å½±å“
        complexity_reliability_impact = 0
        if metrics.cyclomatic_complexity > 20:
            complexity_reliability_impact = (metrics.cyclomatic_complexity - 20) * 1.5
        
        final_score = base_score - error_penalty - warning_penalty - quality_penalty - complexity_reliability_impact
        return max(0, min(100, final_score))
    
    def _calculate_security_score(self, static_results: List[StaticAnalysisResult]) -> float:
        """è®¡ç®—å®‰å…¨æ€§è¯„åˆ†"""
        base_score = 100.0
        
        security_issues = [r for r in static_results if r.category == 'security']
        
        # å®‰å…¨é—®é¢˜æƒ©ç½š
        security_penalty = 0
        for issue in security_issues:
            if issue.severity == 'error':
                security_penalty += 25  # ä¸¥é‡å®‰å…¨é—®é¢˜
            elif issue.severity == 'warning':
                security_penalty += 10  # ä¸­ç­‰å®‰å…¨é—®é¢˜
            else:
                security_penalty += 3   # è½»å¾®å®‰å…¨é—®é¢˜
        
        # ç‰¹å®šå®‰å…¨è§„åˆ™çš„é¢å¤–æƒ©ç½š
        high_risk_patterns = ['B601', 'B602', 'B301']  # Bandité«˜é£é™©è§„åˆ™
        for issue in security_issues:
            if issue.rule_id in high_risk_patterns:
                security_penalty += 15
        
        final_score = base_score - security_penalty
        return max(0, min(100, final_score))
    
    def _calculate_performance_score(
        self, 
        metrics: CodeMetrics,
        static_results: List[StaticAnalysisResult]
    ) -> float:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        base_score = 100.0
        
        # æ€§èƒ½ç›¸å…³é—®é¢˜æƒ©ç½š
        performance_issues = [r for r in static_results if r.category == 'performance']
        performance_penalty = len(performance_issues) * 5
        
        # å¤æ‚åº¦å¯¹æ€§èƒ½çš„å½±å“
        complexity_performance_impact = 0
        if metrics.cyclomatic_complexity > 15:
            complexity_performance_impact = (metrics.cyclomatic_complexity - 15) * 2
        
        # åµŒå¥—æ·±åº¦å½±å“æ€§èƒ½
        nesting_impact = 0
        if metrics.max_nesting_depth > 4:
            nesting_impact = (metrics.max_nesting_depth - 4) * 3
        
        # é•¿å‡½æ•°å¯èƒ½å½±å“æ€§èƒ½
        long_function_impact = len(metrics.long_functions) * 2
        
        final_score = base_score - performance_penalty - complexity_performance_impact - nesting_impact - long_function_impact
        return max(0, min(100, final_score))
    
    def _calculate_style_score(self, static_results: List[StaticAnalysisResult]) -> float:
        """è®¡ç®—é£æ ¼è¯„åˆ†"""
        base_score = 100.0
        
        style_issues = [r for r in static_results if r.category == 'style']
        
        # é£æ ¼é—®é¢˜æƒ©ç½š (ç›¸å¯¹è½»å¾®)
        style_penalty = 0
        for issue in style_issues:
            if issue.severity == 'error':
                style_penalty += 8
            elif issue.severity == 'warning':
                style_penalty += 3
            else:
                style_penalty += 1
        
        final_score = base_score - style_penalty
        return max(0, min(100, final_score))
    
    def _calculate_complexity_score(self, metrics: CodeMetrics) -> float:
        """è®¡ç®—å¤æ‚åº¦è¯„åˆ†"""
        # å¾ªç¯å¤æ‚åº¦è¯„åˆ†
        cc_score = self._score_by_threshold(
            metrics.cyclomatic_complexity,
            self.thresholds['cyclomatic_complexity']
        ) * self.weights.cyclomatic_complexity_weight
        
        # è®¤çŸ¥å¤æ‚åº¦è¯„åˆ†
        cog_score = self._score_by_threshold(
            metrics.cognitive_complexity,
            self.thresholds['cognitive_complexity']
        ) * self.weights.cognitive_complexity_weight
        
        # åµŒå¥—æ·±åº¦è¯„åˆ†
        nest_score = self._score_by_threshold(
            metrics.max_nesting_depth,
            self.thresholds['nesting_depth']
        ) * self.weights.nesting_depth_weight
        
        # åŠ æƒå¹³å‡
        weighted_score = (cc_score + cog_score + nest_score) / (
            self.weights.cyclomatic_complexity_weight +
            self.weights.cognitive_complexity_weight +
            self.weights.nesting_depth_weight
        )
        
        return max(0, min(100, weighted_score * 100))
    
    def _score_by_threshold(self, value: float, thresholds: Dict[str, float]) -> float:
        """æ ¹æ®é˜ˆå€¼è®¡ç®—è¯„åˆ† (0-1)"""
        if value <= thresholds['good']:
            return 1.0
        elif value <= thresholds['acceptable']:
            # çº¿æ€§æ’å€¼
            ratio = (value - thresholds['good']) / (thresholds['acceptable'] - thresholds['good'])
            return 1.0 - (ratio * 0.3)  # goodåˆ°acceptableé™30%
        elif value <= thresholds['bad']:
            ratio = (value - thresholds['acceptable']) / (thresholds['bad'] - thresholds['acceptable'])
            return 0.7 - (ratio * 0.5)  # acceptableåˆ°badå†é™50%
        else:
            # è¶…è¿‡badé˜ˆå€¼ï¼Œç»§ç»­ä¸‹é™
            excess_ratio = min(2.0, (value - thresholds['bad']) / thresholds['bad'])
            return max(0.0, 0.2 - (excess_ratio * 0.2))
    
    def _calculate_weighted_total(self, category_scores: Dict[QualityCategory, float]) -> float:
        """è®¡ç®—åŠ æƒæ€»åˆ†"""
        total = 0.0
        total += category_scores[QualityCategory.MAINTAINABILITY] * self.weights.maintainability
        total += category_scores[QualityCategory.RELIABILITY] * self.weights.reliability
        total += category_scores[QualityCategory.SECURITY] * self.weights.security
        total += category_scores[QualityCategory.PERFORMANCE] * self.weights.performance
        total += category_scores[QualityCategory.STYLE] * self.weights.style
        total += category_scores[QualityCategory.COMPLEXITY] * self.weights.complexity
        
        return total
    
    def _apply_bonuses_and_penalties(
        self, 
        base_score: float, 
        metrics: CodeMetrics, 
        static_results: List[StaticAnalysisResult]
    ) -> float:
        """åº”ç”¨å¥–åŠ±å’Œæƒ©ç½š"""
        adjusted_score = base_score
        
        # æœ€ä½³å®è·µå¥–åŠ±
        if self._has_good_practices(metrics):
            adjusted_score += self.weights.good_practices_bonus
        
        # ç±»å‹å®‰å…¨å¥–åŠ± (TypeScript, Python type hints)
        if self._has_type_safety(metrics, static_results):
            adjusted_score += 3
        
        # æ–‡æ¡£å®Œæ•´æ€§å¥–åŠ±
        if self._has_good_documentation(metrics):
            adjusted_score += 2
        
        return adjusted_score
    
    def _has_good_practices(self, metrics: CodeMetrics) -> bool:
        """æ£€æŸ¥æ˜¯å¦éµå¾ªæœ€ä½³å®è·µ"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºå¤æ‚åº¦å’Œç»“æ„
        return (
            metrics.cyclomatic_complexity <= 10 and
            metrics.max_nesting_depth <= 3 and
            len(metrics.long_functions) == 0
        )
    
    def _has_type_safety(self, metrics: CodeMetrics, static_results: List[StaticAnalysisResult]) -> bool:
        """æ£€æŸ¥ç±»å‹å®‰å…¨"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºæ–‡ä»¶æ‰©å±•åå’Œé™æ€åˆ†æç»“æœ
        has_types = (
            metrics.language in ['typescript', 'java', 'rust', 'go'] or
            (metrics.language == 'python' and any('type' in r.message.lower() for r in static_results))
        )
        return has_types
    
    def _has_good_documentation(self, metrics: CodeMetrics) -> bool:
        """æ£€æŸ¥æ–‡æ¡£è´¨é‡"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºå‡½æ•°æ•°é‡å’Œå¤æ‚åº¦çš„åˆç†æ€§
        if metrics.function_count == 0:
            return True  # ç®€å•è„šæœ¬æ— éœ€è¿‡å¤šæ–‡æ¡£
        
        # å¤æ‚å‡½æ•°åº”è¯¥æœ‰æ–‡æ¡£
        return len(metrics.long_functions) == 0 or metrics.function_count > 5
    
    def _identify_priority_issues(
        self, 
        quality_issues: List[QualityIssue],
        static_results: List[StaticAnalysisResult]
    ) -> List[str]:
        """è¯†åˆ«ä¼˜å…ˆä¿®å¤çš„é—®é¢˜"""
        priority_issues = []
        
        # å®‰å…¨é—®é¢˜æœ€é«˜ä¼˜å…ˆçº§
        security_issues = [r for r in static_results if r.category == 'security' and r.severity == 'error']
        for issue in security_issues[:3]:  # æœ€å¤š3ä¸ª
            priority_issues.append(f"ğŸš¨ Security: {issue.message}")
        
        # å¤æ‚åº¦é—®é¢˜
        complexity_issues = [i for i in quality_issues if i.category == 'complexity' and i.severity == 'error']
        for issue in complexity_issues[:2]:  # æœ€å¤š2ä¸ª
            priority_issues.append(f"âš¡ Complexity: {issue.message}")
        
        # å¯é æ€§é—®é¢˜
        reliability_issues = [r for r in static_results if r.severity == 'error'][:2]
        for issue in reliability_issues:
            priority_issues.append(f"âš ï¸  Reliability: {issue.message}")
        
        return priority_issues
    
    def _generate_recommendations(
        self, 
        metrics: CodeMetrics,
        static_results: List[StaticAnalysisResult]
    ) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # å¤æ‚åº¦å»ºè®®
        if metrics.cyclomatic_complexity > 15:
            recommendations.append(
                f"Reduce cyclomatic complexity from {metrics.cyclomatic_complexity} to â‰¤10 by extracting functions"
            )
        
        if metrics.cognitive_complexity > 25:
            recommendations.append(
                f"Simplify logic to reduce cognitive complexity from {metrics.cognitive_complexity} to â‰¤15"
            )
        
        # ç»“æ„å»ºè®®
        if len(metrics.long_functions) > 0:
            recommendations.append(
                f"Break down {len(metrics.long_functions)} long functions into smaller, focused functions"
            )
        
        if len(metrics.duplicated_code_blocks) > 0:
            recommendations.append(
                "Extract common code patterns into reusable functions to eliminate duplication"
            )
        
        # å®‰å…¨å»ºè®®
        security_issues = [r for r in static_results if r.category == 'security']
        if security_issues:
            recommendations.append(
                f"Address {len(security_issues)} security issues detected by static analysis"
            )
        
        return recommendations[:5]  # æœ€å¤š5ä¸ªå»ºè®®
    
    def _identify_strengths(
        self, 
        metrics: CodeMetrics,
        category_scores: Dict[QualityCategory, float]
    ) -> List[str]:
        """è¯†åˆ«ä»£ç ä¼˜åŠ¿"""
        strengths = []
        
        # åŸºäºåˆ†ç±»è¯„åˆ†è¯†åˆ«ä¼˜åŠ¿
        for category, score in category_scores.items():
            if score >= 85:
                strengths.append(f"Excellent {category.value}")
        
        # å…·ä½“ä¼˜åŠ¿
        if metrics.cyclomatic_complexity <= 5:
            strengths.append("Low complexity - easy to understand")
        
        if metrics.max_nesting_depth <= 2:
            strengths.append("Minimal nesting - clean structure")
        
        if len(metrics.long_functions) == 0:
            strengths.append("Well-sized functions")
        
        if len(metrics.duplicated_code_blocks) == 0:
            strengths.append("No code duplication")
        
        return strengths
    
    def calculate_comprehensive_score(
        self, 
        metrics: CodeMetrics, 
        static_results: List[StaticAnalysisResult],
        quality_issues: List[QualityIssue] = None
    ) -> QualityScore:
        """è®¡ç®—ç»¼åˆè´¨é‡è¯„åˆ† (å…¼å®¹æ–¹æ³•)
        
        è¿™æ˜¯ calculate_quality_score çš„åˆ«åæ–¹æ³•ï¼Œä¸ºäº†å‘åå…¼å®¹
        """
        if quality_issues is None:
            quality_issues = []
            
        return self.calculate_quality_score(metrics, static_results, quality_issues)
    
    def _estimate_maintainability_index(self, metrics: CodeMetrics) -> float:
        """ä¼°ç®—ç»´æŠ¤æ€§æŒ‡æ•°"""
        # ç®€åŒ–çš„ç»´æŠ¤æ€§æŒ‡æ•°è®¡ç®—
        base_score = 100
        
        # å¤æ‚åº¦æƒ©ç½š
        complexity_penalty = metrics.cyclomatic_complexity * 2
        cognitive_penalty = metrics.cognitive_complexity * 1.5
        
        # ä»£ç é‡æƒ©ç½š
        loc_penalty = max(0, (metrics.lines_of_code - 100) * 0.1)
        
        # ç»“æ„é—®é¢˜æƒ©ç½š
        structure_penalty = len(metrics.long_functions) * 5 + len(metrics.duplicated_code_blocks) * 8
        
        score = base_score - complexity_penalty - cognitive_penalty - loc_penalty - structure_penalty
        return max(0, min(100, score))
