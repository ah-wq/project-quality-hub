"""æ™ºèƒ½å¢é‡æ›´æ–°ç³»ç»Ÿï¼Œæä¾›æ–‡ä»¶å˜æ›´ç›‘æ§ä¸çŸ¥è¯†å›¾è°±å¢é‡åˆ·æ–°èƒ½åŠ›ã€‚"""

from __future__ import annotations

import hashlib
import logging
import os
import subprocess
import threading
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional, Set

from watchdog.events import FileSystemEventHandler
from watchdog.observers import Observer
from watchdog.observers.polling import PollingObserver

from .project_mind import FileNode, ProjectKnowledgeGraph
from .project_memory import ProjectMemoryManager

logger = logging.getLogger(__name__)

@dataclass
class FileChangeInfo:
    """æ–‡ä»¶å˜æ›´ä¿¡æ¯"""
    file_path: str
    change_type: str  # 'created', 'modified', 'deleted', 'moved'
    old_hash: Optional[str] = None
    new_hash: Optional[str] = None
    timestamp: datetime = None
    content_changed: bool = False
    metadata_changed: bool = False

class SmartFileHandler(FileSystemEventHandler):
    """æ™ºèƒ½æ–‡ä»¶ç›‘æ§å¤„ç†å™¨"""

    def __init__(self, update_manager):
        self.update_manager = update_manager
        self.batch_changes: Dict[str, FileChangeInfo] = {}
        self.batch_timer = None
        self.batch_delay = 2.0  # 2ç§’æ‰¹å¤„ç†å»¶è¿Ÿ

    def on_modified(self, event):
        if not event.is_directory:
            self._queue_change(event.src_path, 'modified')

    def on_created(self, event):
        if not event.is_directory:
            self._queue_change(event.src_path, 'created')

    def on_deleted(self, event):
        if not event.is_directory:
            self._queue_change(event.src_path, 'deleted')

    def on_moved(self, event):
        if not event.is_directory:
            # å¤„ç†æ–‡ä»¶ç§»åŠ¨/é‡å‘½å
            self._queue_change(event.src_path, 'deleted')
            self._queue_change(event.dest_path, 'created')

    def _queue_change(self, file_path: str, change_type: str):
        """å°†æ–‡ä»¶å˜æ›´åŠ å…¥é˜Ÿåˆ—"""
        if not self.update_manager._should_monitor_file(file_path):
            return

        file_path = str(Path(file_path).resolve())

        # è®¡ç®—æ–‡ä»¶hash
        new_hash = None
        if change_type != 'deleted' and Path(file_path).exists():
            try:
                new_hash = self.update_manager._calculate_file_hash(file_path)
            except Exception:
                pass

        change_info = FileChangeInfo(
            file_path=file_path,
            change_type=change_type,
            new_hash=new_hash,
            timestamp=datetime.now()
        )

        self.batch_changes[file_path] = change_info

        # é‡ç½®æ‰¹å¤„ç†å®šæ—¶å™¨
        if self.batch_timer:
            self.batch_timer.cancel()

        self.batch_timer = threading.Timer(self.batch_delay, self._process_batch_changes)
        self.batch_timer.start()

    def _process_batch_changes(self):
        """æ‰¹å¤„ç†æ–‡ä»¶å˜æ›´"""
        if self.batch_changes:
            changes = dict(self.batch_changes)
            self.batch_changes.clear()

            # å¼‚æ­¥å¤„ç†å˜æ›´
            threading.Thread(
                target=self.update_manager._process_file_changes,
                args=(changes,),
                daemon=True
            ).start()

class SmartIncrementalUpdater:
    """æ™ºèƒ½å¢é‡æ›´æ–°ç®¡ç†å™¨"""

    def __init__(self, project_root: str):
        self.project_root = Path(project_root).absolute()
        self.memory_manager = ProjectMemoryManager()
        self.observer = None
        self.monitoring = False
        self._observer_class = None

        # å¿½ç•¥çš„æ–‡ä»¶æ¨¡å¼
        self.ignore_patterns = {
            '.git', '.DS_Store', '__pycache__', 'node_modules',
            '.next', '.nuxt', 'dist', 'build', '.vscode',
            '*.log', '*.tmp', '*.temp', '*.cache'
        }

        # æ”¯æŒçš„ä»£ç æ–‡ä»¶æ‰©å±•å
        self.code_extensions = {
            '.py', '.js', '.ts', '.tsx', '.jsx', '.vue', '.java',
            '.cpp', '.c', '.h', '.hpp', '.cs', '.go', '.rs',
            '.php', '.rb', '.swift', '.kt', '.scala', '.md',
            '.json', '.yaml', '.yml', '.xml', '.css', '.scss',
            '.less', '.html', '.htm'
        }

    def _should_force_polling(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨è½®è¯¢ç›‘æ§"""
        flag = os.environ.get("WATCHDOG_FORCE_POLLING", "")
        if not flag:
            return False
        return flag.lower() not in {"0", "false", "no"}

    def _candidate_observers(self):
        """ç”Ÿæˆå¯ç”¨çš„ç›‘æ§å®ç°åˆ—è¡¨"""
        if self._should_force_polling():
            return [PollingObserver]
        return [Observer, PollingObserver]

    def _should_monitor_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ç›‘æ§æ­¤æ–‡ä»¶"""
        path = Path(file_path)

        # æ£€æŸ¥æ˜¯å¦åœ¨å¿½ç•¥åˆ—è¡¨ä¸­
        for pattern in self.ignore_patterns:
            if pattern.startswith('*'):
                if path.name.endswith(pattern[1:]):
                    return False
            else:
                if pattern in path.parts:
                    return False

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        return path.suffix.lower() in self.code_extensions

    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹hash"""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
            return hashlib.md5(content).hexdigest()
        except Exception:
            return ""

    def _get_git_file_info(self, file_path: str) -> Dict[str, Any]:
        """è·å–Gitæ–‡ä»¶ä¿¡æ¯"""
        try:
            # è·å–æ–‡ä»¶æœ€åä¿®æ”¹çš„commitä¿¡æ¯
            result = subprocess.run(
                ["git", "log", "-1", "--format=%H|%ct|%an", "--", file_path],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                check=True
            )

            if result.stdout.strip():
                hash_str, timestamp, author = result.stdout.strip().split("|")
                return {
                    'last_commit_hash': hash_str,
                    'last_commit_time': datetime.fromtimestamp(int(timestamp)),
                    'last_author': author
                }
        except (subprocess.CalledProcessError, FileNotFoundError, ValueError):
            pass

        return {}

    def _analyze_content_changes(self, file_path: str, old_hash: str, new_hash: str) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶å†…å®¹å…·ä½“å˜æ›´"""
        if old_hash == new_hash:
            return {'content_changed': False}

        changes = {
            'content_changed': True,
            'hash_changed': True,
            'lines_changed': 0,
            'entities_affected': [],
            'risk_level': 'low'
        }

        try:
            # ä½¿ç”¨git diffåˆ†æå˜æ›´
            result = subprocess.run(
                ["git", "diff", "--numstat", "HEAD~1", "HEAD", "--", file_path],
                cwd=self.project_root,
                capture_output=True,
                text=True
            )

            if result.stdout.strip():
                lines = result.stdout.strip().split('\t')
                if len(lines) >= 2:
                    added = int(lines[0]) if lines[0] != '-' else 0
                    deleted = int(lines[1]) if lines[1] != '-' else 0
                    changes['lines_changed'] = added + deleted

                    # è¯„ä¼°é£é™©çº§åˆ«
                    if changes['lines_changed'] > 100:
                        changes['risk_level'] = 'high'
                    elif changes['lines_changed'] > 20:
                        changes['risk_level'] = 'medium'

        except (subprocess.CalledProcessError, ValueError):
            pass

        return changes

    def _process_file_changes(self, changes: Dict[str, FileChangeInfo]):
        """å¤„ç†æ‰¹é‡æ–‡ä»¶å˜æ›´"""
        logger.info(f"ğŸ”„ å¤„ç† {len(changes)} ä¸ªæ–‡ä»¶å˜æ›´")

        # åŠ è½½å½“å‰é¡¹ç›®å›¾è°±
        knowledge_graph = self.memory_manager.load_project(str(self.project_root))
        if not knowledge_graph:
            logger.warning("æ— æ³•åŠ è½½é¡¹ç›®å›¾è°±ï¼Œæ‰§è¡Œå®Œæ•´é‡æ–°åˆ†æ")
            self.full_reanalysis()
            return

        # åˆ†ææ¯ä¸ªå˜æ›´çš„æ–‡ä»¶
        updated_files = set()
        affected_entities = set()

        for file_path, change_info in changes.items():
            try:
                self._process_single_file_change(knowledge_graph, change_info)
                updated_files.add(file_path)

                # åˆ†æå½±å“çš„å®ä½“
                if file_path in knowledge_graph.files:
                    file_entities = knowledge_graph.files[file_path].entities
                    affected_entities.update(entity.name for entity in file_entities)

            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶å˜æ›´å¤±è´¥ {file_path}: {e}")

        # æ›´æ–°ä¾èµ–å…³ç³»
        if updated_files:
            self._update_dependencies(knowledge_graph, updated_files)

        # ä¿å­˜æ›´æ–°åçš„å›¾è°±
        if self.memory_manager.save_project(knowledge_graph):
            logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: {len(updated_files)} ä¸ªæ–‡ä»¶, {len(affected_entities)} ä¸ªå®ä½“")
        else:
            logger.error("âŒ å¢é‡æ›´æ–°ä¿å­˜å¤±è´¥")

    def _process_single_file_change(self, knowledge_graph: ProjectKnowledgeGraph, change_info: FileChangeInfo):
        """å¤„ç†å•ä¸ªæ–‡ä»¶çš„å˜æ›´"""
        file_path = change_info.file_path

        if change_info.change_type == 'deleted':
            # åˆ é™¤æ–‡ä»¶
            if file_path in knowledge_graph.files:
                del knowledge_graph.files[file_path]

            # åˆ é™¤ç›¸å…³å®ä½“
            entities_to_remove = [
                name for name, entity in knowledge_graph.entities.items()
                if entity.file_path == file_path
            ]
            for entity_name in entities_to_remove:
                del knowledge_graph.entities[entity_name]

        elif change_info.change_type in ['created', 'modified']:
            # é‡æ–°åˆ†ææ–‡ä»¶
            if Path(file_path).exists():
                # è·å–Gitä¿¡æ¯
                git_info = self._get_git_file_info(file_path)

                # åˆ†ææ–‡ä»¶å†…å®¹å˜æ›´
                old_hash = knowledge_graph.files.get(file_path, FileNode("", "", 0, 0, datetime.now(), "")).file_hash
                change_summary = self._analyze_content_changes(
                    file_path, old_hash, change_info.new_hash or ""
                )

                # é‡æ–°åˆ†ææ–‡ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
                relative_path = str(Path(file_path).relative_to(self.project_root))
                knowledge_graph._analyze_single_file(file_path, relative_path)

                # æ›´æ–°Gitä¿¡æ¯
                if file_path in knowledge_graph.files and git_info:
                    file_node = knowledge_graph.files[file_path]
                    if 'last_commit_time' in git_info:
                        file_node.last_modified = git_info['last_commit_time']
                if change_summary.get("risk_level") in {"medium", "high"}:
                    logger.warning(
                        "æ–‡ä»¶ %s æ£€æµ‹åˆ° %s é£é™©çº§åˆ«çš„å†…å®¹å˜æ›´ (è¡Œå˜åŠ¨: %s)",
                        file_path,
                        change_summary["risk_level"],
                        change_summary.get("lines_changed", 0),
                    )

    def _update_dependencies(self, knowledge_graph: ProjectKnowledgeGraph, updated_files: Set[str]):
        """æ›´æ–°å—å½±å“æ–‡ä»¶çš„ä¾èµ–å…³ç³»"""
        # é‡æ–°è®¡ç®—ä¾èµ–å…³ç³»ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for file_path in updated_files:
            if file_path in knowledge_graph.files:
                file_node = knowledge_graph.files[file_path]
                # é‡æ–°åˆ†æå¯¼å…¥å’Œå¯¼å‡º
                knowledge_graph._analyze_imports_exports(file_path, file_node)

    def start_monitoring(self):
        """å¼€å§‹å®æ—¶ç›‘æ§"""
        if self.monitoring:
            logger.warning("ç›‘æ§å·²åœ¨è¿è¡Œ")
            return

        logger.info("ğŸ” å¼€å§‹ç›‘æ§é¡¹ç›®: %s", self.project_root)

        event_handler = SmartFileHandler(self)
        observer = None
        last_error: Exception | None = None

        for observer_cls in self._candidate_observers():
            candidate = None
            try:
                candidate = observer_cls()
                candidate.schedule(event_handler, str(self.project_root), recursive=True)
                candidate.start()
            except Exception as exc:
                last_error = exc
                logger.warning("æ— æ³•ä½¿ç”¨%så¯åŠ¨æ–‡ä»¶ç›‘æ§: %s", observer_cls.__name__, exc)
                if candidate is not None:
                    try:
                        candidate.stop()
                    except Exception:
                        pass
            else:
                observer = candidate
                self._observer_class = observer_cls
                break

        if observer is None:
            message = "é»˜è®¤ç›‘æ§ä¸è½®è¯¢ç›‘æ§å‡æ— æ³•å¯åŠ¨" if isinstance(last_error, Exception) else "æ— æ³•å¯åŠ¨æ–‡ä»¶ç›‘æ§"
            raise RuntimeError(f"{message}: {last_error}") from last_error

        self.observer = observer
        self.monitoring = True
        logger.info("âœ… å®æ—¶ç›‘æ§å·²å¯åŠ¨ï¼ˆæ¨¡å¼: %sï¼‰", self._observer_class.__name__)

    def stop_monitoring(self):
        """åœæ­¢å®æ—¶ç›‘æ§"""
        if not self.monitoring or not self.observer:
            return

        logger.info("ğŸ›‘ åœæ­¢é¡¹ç›®ç›‘æ§")

        self.observer.stop()
        self.observer.join()
        self.monitoring = False

        logger.info("âœ… ç›‘æ§å·²åœæ­¢")

    def full_reanalysis(self):
        """æ‰§è¡Œå®Œæ•´é‡æ–°åˆ†æ"""
        logger.info("ğŸ”„ æ‰§è¡Œå®Œæ•´é¡¹ç›®é‡æ–°åˆ†æ")

        knowledge_graph = ProjectKnowledgeGraph(str(self.project_root))
        knowledge_graph.analyze_project()

        if self.memory_manager.save_project(knowledge_graph):
            logger.info("âœ… å®Œæ•´é‡æ–°åˆ†æå®Œæˆ")
            return True
        else:
            logger.error("âŒ å®Œæ•´é‡æ–°åˆ†æå¤±è´¥")
            return False

    def force_update(self) -> Dict[str, Any]:
        """å¼ºåˆ¶æ›´æ–°é¡¹ç›®å›¾è°±"""
        logger.info("ğŸ”§ å¼ºåˆ¶æ›´æ–°é¡¹ç›®å›¾è°±")

        # åœæ­¢ç›‘æ§
        was_monitoring = self.monitoring
        if was_monitoring:
            self.stop_monitoring()

        # æ‰§è¡Œå®Œæ•´åˆ†æ
        success = self.full_reanalysis()

        # é‡å¯ç›‘æ§
        if was_monitoring:
            self.start_monitoring()

        return {
            'status': 'success' if success else 'failed',
            'timestamp': datetime.now(),
            'monitoring_restarted': was_monitoring
        }

    def get_update_status(self) -> Dict[str, Any]:
        """è·å–æ›´æ–°çŠ¶æ€"""
        knowledge_graph = self.memory_manager.load_project(str(self.project_root))

        return {
            'monitoring': self.monitoring,
            'project_root': str(self.project_root),
            'last_analysis': knowledge_graph.context.last_analysis if knowledge_graph and knowledge_graph.context else None,
            'files_count': len(knowledge_graph.files) if knowledge_graph else 0,
            'entities_count': len(knowledge_graph.entities) if knowledge_graph else 0,
            'supported_extensions': list(self.code_extensions),
            'ignore_patterns': list(self.ignore_patterns)
        }
