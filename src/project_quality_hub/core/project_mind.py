"""SuperClaude ProjectMind System - Êô∫ËÉΩÈ°πÁõÆËÆ∞ÂøÜÁ≥ªÁªü

Ê†∏ÂøÉÂäüËÉΩ:
1. È°πÁõÆÁü•ËØÜÂõæË∞±ÊûÑÂª∫ÂíåÁª¥Êä§
2. ‰ª£Á†Å‰æùËµñÂÖ≥Á≥ªÊ∑±Â∫¶ÂàÜÊûê  
3. È°πÁõÆ‰∏ä‰∏ãÊñáÊô∫ËÉΩÁêÜËß£
4. ‰ª£Á†ÅÂèòÊõ¥ÂΩ±ÂìçÊô∫ËÉΩÈ¢ÑÊµã
5. È°πÁõÆËÆ∞ÂøÜÊåÅ‰πÖÂåñÂ≠òÂÇ®

ËÆ©ClaudeËé∑ÂæóÈáèÂ≠êÁ∫ßÂà´ÁöÑÈ°πÁõÆÁêÜËß£ËÉΩÂäõÔºåËøúË∂ÖÂÖ∂‰ªñClaude Code
"""

from __future__ import annotations

import ast
import hashlib
import json
import logging
import re
from collections import defaultdict
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import networkx as nx

logger = logging.getLogger(__name__)


@dataclass
class CodeEntity:
    """‰ª£Á†ÅÂÆû‰Ωì - ÂáΩÊï∞„ÄÅÁ±ª„ÄÅÂèòÈáèÁ≠â"""
    name: str
    entity_type: str  # 'function', 'class', 'variable', 'import', 'interface'
    file_path: str
    line_number: int
    signature: Optional[str] = None
    docstring: Optional[str] = None
    complexity_score: float = 0.0
    usage_count: int = 0
    last_modified: datetime = field(default_factory=datetime.now)
    dependencies: List[str] = field(default_factory=list)
    dependents: List[str] = field(default_factory=list)


@dataclass 
class FileNode:
    """Êñá‰ª∂ËäÇÁÇπ"""
    file_path: str
    language: str
    size_bytes: int
    line_count: int
    last_modified: datetime
    file_hash: str
    imports: List[str] = field(default_factory=list)
    exports: List[str] = field(default_factory=list)  
    entities: List[CodeEntity] = field(default_factory=list)
    risk_score: float = 0.0
    change_frequency: int = 0


@dataclass
class ProjectContext:
    """È°πÁõÆ‰∏ä‰∏ãÊñá‰ø°ÊÅØ"""
    project_root: str
    project_name: str
    framework_type: str  # 'react', 'vue', 'node', 'python', 'java', etc.
    main_language: str
    architecture_pattern: str  # 'mvc', 'mvvm', 'microservices', 'monorepo'
    build_system: str  # 'webpack', 'vite', 'gradle', 'maven', etc.
    package_manager: str  # 'npm', 'yarn', 'pip', 'maven'
    version: str
    last_analysis: datetime = field(default_factory=datetime.now)
    total_files: int = 0
    total_lines: int = 0
    complexity_distribution: Dict[str, int] = field(default_factory=dict)


@dataclass
class DependencyRelation:
    """‰æùËµñÂÖ≥Á≥ª"""
    from_entity: str
    to_entity: str
    relation_type: str  # 'imports', 'calls', 'extends', 'implements', 'uses'
    strength: float  # ‰æùËµñÂº∫Â∫¶ 0-1
    file_path: str
    line_number: int


class ProjectKnowledgeGraph:
    """È°πÁõÆÁü•ËØÜÂõæË∞± - ProjectMindÁöÑÊ†∏ÂøÉ"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root).absolute()
        self.graph = nx.DiGraph()  # ÊúâÂêëÂõæÂ≠òÂÇ®‰æùËµñÂÖ≥Á≥ª
        self.files: Dict[str, FileNode] = {}
        self.entities: Dict[str, CodeEntity] = {}
        self.context: Optional[ProjectContext] = None
        self.change_history: List[Dict] = []
        
        # ÊîØÊåÅÁöÑÊñá‰ª∂Á±ªÂûãÂíåËØ≠Ë®Ä
        self.supported_extensions = {
            '.py': 'python',
            '.js': 'javascript', 
            '.jsx': 'javascript',
            '.ts': 'typescript',
            '.tsx': 'typescript',
            '.java': 'java',
            '.go': 'go',
            '.rs': 'rust',
            '.c': 'c',
            '.cpp': 'cpp',
            '.h': 'c',
            '.hpp': 'cpp'
        }
        
        # Ê°ÜÊû∂ËØÜÂà´Ê®°Âºè
        self.framework_patterns = {
            'react': ['react', 'jsx', 'tsx', 'package.json'],
            'vue': ['vue', '@vue', 'vue-cli'],
            'angular': ['@angular', 'angular.json'],
            'node': ['package.json', 'node_modules'],
            'django': ['django', 'manage.py', 'settings.py'],
            'flask': ['flask', 'app.py'],
            'spring': ['spring', 'pom.xml', '@SpringBootApplication'],
            'next': ['next', 'next.config.js']
        }
    
    def analyze_project(self, max_files: int = 1000) -> ProjectContext:
        """ÂÆåÊï¥È°πÁõÆÂàÜÊûê - ÊûÑÂª∫Áü•ËØÜÂõæË∞±"""
        logger.info("üß† ProjectMind ÂºÄÂßãÊ∑±Â∫¶È°πÁõÆÂàÜÊûê...")
        
        # 1. È°πÁõÆÂü∫Á°Ä‰ø°ÊÅØÂàÜÊûê
        context = self._analyze_project_context()
        self.context = context
        
        # 2. Êñá‰ª∂Êâ´ÊèèÂíåÂàÜÊûê
        files_analyzed = self._scan_and_analyze_files(max_files)
        logger.info("üìÅ ÂàÜÊûê‰∫Ü %s ‰∏™Êñá‰ª∂", files_analyzed)
        
        # 3. ÊûÑÂª∫‰æùËµñÂÖ≥Á≥ªÂõæ
        dependency_count = self._build_dependency_graph()
        logger.info("üîó ÊûÑÂª∫‰∫Ü %s ‰∏™‰æùËµñÂÖ≥Á≥ª", dependency_count)
        
        # 4. ÂÆû‰ΩìÂÖ≥Á≥ªÂàÜÊûê
        entities = self._analyze_entity_relationships()
        logger.info("üß© ËØÜÂà´‰∫Ü %s ‰∏™‰ª£Á†ÅÂÆû‰Ωì", entities)
        
        # 5. È£éÈô©ËØÑ‰º∞ÂíåÂ§çÊùÇÂ∫¶ÂàÜÊûê
        self._calculate_risk_scores()
        
        # 6. Êõ¥Êñ∞‰∏ä‰∏ãÊñáÁªüËÆ°
        self._update_context_statistics()
        
        logger.info(
            "‚úÖ ProjectMind ÂàÜÊûêÂÆåÊàê! È°πÁõÆ: %s | Ê°ÜÊû∂: %s | ËØ≠Ë®Ä: %s | Êñá‰ª∂: %s | ‰ª£Á†ÅË°å: %s",
            context.project_name,
            context.framework_type,
            context.main_language,
            context.total_files,
            context.total_lines,
        )
        
        return context

    def analyze_changed_files(self, changed_files):
        """‰ªÖÂàÜÊûêÊåáÂÆöÁöÑÂ∑≤Êõ¥ÊîπÊñá‰ª∂ÔºåÁî®‰∫éÂ¢ûÈáèÂàÜÊûê"""
        logger.info("üß† ProjectMind Â¢ûÈáèÂàÜÊûê %s ‰∏™Êõ¥ÊîπÁöÑÊñá‰ª∂...", len(changed_files))
        
        # 1. Â¶ÇÊûúÊ≤°ÊúâÁé∞Êúâ‰∏ä‰∏ãÊñáÔºåÂÖàËøõË°åÂü∫Á°ÄÈ°πÁõÆÂàÜÊûê
        if not self.context:
            self.context = self._analyze_project_context()
        
        # 2. ‰ªÖÂàÜÊûêÊõ¥ÊîπÁöÑÊñá‰ª∂
        files_analyzed = 0
        for file_path in changed_files:
            full_path = Path(self.project_root) / file_path
            if full_path.exists() and full_path.is_file():
                file_node = self._analyze_single_file(full_path)
                if file_node:
                    # üîß ‰øÆÂ§çÔºöÂ∞ÜÂàÜÊûêÁöÑÊñá‰ª∂Ê∑ªÂä†Âà∞self.files‰∏≠
                    self.files[file_node.file_path] = file_node
                    files_analyzed += 1
        
        logger.info("üìÅ Â¢ûÈáèÂàÜÊûê‰∫Ü %s ‰∏™Êõ¥ÊîπÊñá‰ª∂", files_analyzed)
        
        # 3. Êõ¥Êñ∞‰æùËµñÂÖ≥Á≥ªÔºà‰ªÖÊ∂âÂèäÊõ¥ÊîπÊñá‰ª∂ÁöÑÈÉ®ÂàÜÔºâ
        dependency_count = self._build_dependency_graph()
        logger.info("üîó Êõ¥Êñ∞‰æùËµñÂÖ≥Á≥ªÊï∞Èáè: %s", dependency_count)
        
        # 4. Êõ¥Êñ∞ÁªüËÆ°‰ø°ÊÅØ
        self._update_context_statistics()
        
        logger.info("‚úÖ ProjectMind Â¢ûÈáèÂàÜÊûêÂÆåÊàê!")
        return self.context

    def _analyze_project_context(self) -> ProjectContext:
        """ÂàÜÊûêÈ°πÁõÆ‰∏ä‰∏ãÊñá‰ø°ÊÅØ"""
        project_name = self.project_root.name
        
        # Ê£ÄÊµãÊ°ÜÊû∂Á±ªÂûã
        framework_type = self._detect_framework_type()
        
        # Ê£ÄÊµã‰∏ªË¶ÅÁºñÁ®ãËØ≠Ë®Ä
        main_language = self._detect_main_language()
        
        # Ê£ÄÊµãÊû∂ÊûÑÊ®°Âºè
        architecture_pattern = self._detect_architecture_pattern()
        
        # Ê£ÄÊµãÊûÑÂª∫Á≥ªÁªü
        build_system = self._detect_build_system()
        
        # Ê£ÄÊµãÂåÖÁÆ°ÁêÜÂô®
        package_manager = self._detect_package_manager()
        
        # Ëé∑ÂèñÁâàÊú¨‰ø°ÊÅØ
        version = self._get_project_version()
        
        return ProjectContext(
            project_root=str(self.project_root),
            project_name=project_name,
            framework_type=framework_type,
            main_language=main_language,
            architecture_pattern=architecture_pattern,
            build_system=build_system,
            package_manager=package_manager,
            version=version
        )
    
    def _detect_framework_type(self) -> str:
        """Ê£ÄÊµãÊ°ÜÊû∂Á±ªÂûã"""
        # Ê£ÄÊü•package.json
        package_json = self.project_root / "package.json"
        if package_json.exists():
            try:
                with open(package_json, 'r') as f:
                    data = json.load(f)
                    deps = {**data.get('dependencies', {}), **data.get('devDependencies', {})}
                    
                    if 'react' in deps or 'next' in deps:
                        return 'next' if 'next' in deps else 'react'
                    elif '@vue/core' in deps or 'vue' in deps:
                        return 'vue'
                    elif '@angular/core' in deps:
                        return 'angular'
                    elif 'express' in deps:
                        return 'express'
                    else:
                        return 'node'
            except Exception:
                pass
        
        # Ê£ÄÊü•PythonÈ°πÁõÆ
        if (self.project_root / "requirements.txt").exists() or (self.project_root / "pyproject.toml").exists():
            return 'python'
        
        # Ê£ÄÊü•JavaÈ°πÁõÆ
        if (self.project_root / "pom.xml").exists():
            return 'spring'
        
        # Ê£ÄÊü•GoÈ°πÁõÆ
        if (self.project_root / "go.mod").exists():
            return 'go'
        
        return 'unknown'
    
    def _detect_main_language(self) -> str:
        """Ê£ÄÊµã‰∏ªË¶ÅÁºñÁ®ãËØ≠Ë®Ä"""
        language_count = defaultdict(int)
        
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file() and file_path.suffix in self.supported_extensions:
                language = self.supported_extensions[file_path.suffix]
                language_count[language] += 1
        
        if language_count:
            return max(language_count, key=language_count.get)
        
        return 'unknown'
    
    def _detect_architecture_pattern(self) -> str:
        """Ê£ÄÊµãÊû∂ÊûÑÊ®°Âºè"""
        # Ê£ÄÊü•ÁõÆÂΩïÁªìÊûÑ
        dirs = [d.name.lower() for d in self.project_root.iterdir() if d.is_dir()]
        
        if 'packages' in dirs or 'apps' in dirs:
            return 'monorepo'
        elif 'src' in dirs and ('components' in dirs or 'views' in dirs):
            return 'spa'
        elif 'controllers' in dirs and 'models' in dirs and 'views' in dirs:
            return 'mvc'
        elif 'services' in dirs and 'repositories' in dirs:
            return 'layered'
        
        return 'unknown'
    
    def _detect_build_system(self) -> str:
        """Ê£ÄÊµãÊûÑÂª∫Á≥ªÁªü"""
        if (self.project_root / "webpack.config.js").exists():
            return 'webpack'
        elif (self.project_root / "vite.config.js").exists() or (self.project_root / "vite.config.ts").exists():
            return 'vite'
        elif (self.project_root / "pom.xml").exists():
            return 'maven'
        elif (self.project_root / "build.gradle").exists():
            return 'gradle'
        elif (self.project_root / "Makefile").exists():
            return 'make'
        
        return 'unknown'
    
    def _detect_package_manager(self) -> str:
        """Ê£ÄÊµãÂåÖÁÆ°ÁêÜÂô®"""
        if (self.project_root / "yarn.lock").exists():
            return 'yarn'
        elif (self.project_root / "package-lock.json").exists():
            return 'npm'
        elif (self.project_root / "pnpm-lock.yaml").exists():
            return 'pnpm'
        elif (self.project_root / "requirements.txt").exists():
            return 'pip'
        elif (self.project_root / "Pipfile").exists():
            return 'pipenv'
        
        return 'unknown'
    
    def _get_project_version(self) -> str:
        """Ëé∑ÂèñÈ°πÁõÆÁâàÊú¨"""
        # Ê£ÄÊü•package.json
        package_json = self.project_root / "package.json"
        if package_json.exists():
            try:
                with open(package_json, 'r') as f:
                    data = json.load(f)
                    return data.get('version', '0.0.0')
            except Exception:
                pass
        
        # Ê£ÄÊü•pyproject.toml
        pyproject = self.project_root / "pyproject.toml"
        if pyproject.exists():
            try:
                with open(pyproject, 'r') as f:
                    content = f.read()
                    version_match = re.search(r'version\s*=\s*["\']([^"\']+)["\']', content)
                    if version_match:
                        return version_match.group(1)
            except Exception:
                pass
        
        return '0.0.0'
    
    def _scan_and_analyze_files(self, max_files: int) -> int:
        """Êâ´ÊèèÂíåÂàÜÊûêÈ°πÁõÆÊñá‰ª∂"""
        files_processed = 0
        
        # ÂøΩÁï•ÁöÑÁõÆÂΩï
        ignore_dirs = {
            'node_modules', '.git', '__pycache__', '.pytest_cache',
            'dist', 'build', 'target', '.idea', '.vscode', 'coverage'
        }
        
        for file_path in self.project_root.rglob("*"):
            if files_processed >= max_files:
                break
                
            # Ê£ÄÊü•ÊòØÂê¶Â∫îËØ•ÂøΩÁï•
            if any(ignore_dir in file_path.parts for ignore_dir in ignore_dirs):
                continue
                
            if file_path.is_file() and file_path.suffix in self.supported_extensions:
                try:
                    file_node = self._analyze_single_file(file_path)
                    if file_node:
                        self.files[str(file_path)] = file_node
                        files_processed += 1
                except Exception as e:
                    logger.warning("ÂàÜÊûêÊñá‰ª∂Â§±Ë¥• %s: %s", file_path, e)
        
        return files_processed
    
    def _analyze_single_file(self, file_path: Path) -> Optional[FileNode]:
        """ÂàÜÊûêÂçï‰∏™Êñá‰ª∂"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Êñá‰ª∂Âü∫Á°Ä‰ø°ÊÅØ
            stat = file_path.stat()
            file_hash = hashlib.md5(content.encode()).hexdigest()
            language = self.supported_extensions[file_path.suffix]
            
            # Ë°åÊï∞ÁªüËÆ°
            lines = content.split('\n')
            line_count = len([line for line in lines if line.strip()])
            
            file_node = FileNode(
                file_path=str(file_path),
                language=language,
                size_bytes=stat.st_size,
                line_count=line_count,
                last_modified=datetime.fromtimestamp(stat.st_mtime),
                file_hash=file_hash
            )
            
            # ËØ≠Ë®ÄÁâπÂÆöÁöÑÂàÜÊûê
            if language == 'python':
                self._analyze_python_file(file_node, content)
            elif language in ['javascript', 'typescript']:
                self._analyze_js_file(file_node, content)
            
            return file_node
            
        except Exception as e:
            logger.warning("Êñá‰ª∂ÂàÜÊûêÈîôËØØ %s: %s", file_path, e)
            return None
    
    def _analyze_python_file(self, file_node: FileNode, content: str):
        """ÂàÜÊûêPythonÊñá‰ª∂"""
        try:
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    entity = CodeEntity(
                        name=node.name,
                        entity_type='function',
                        file_path=file_node.file_path,
                        line_number=node.lineno,
                        signature=f"def {node.name}(...)",
                        docstring=ast.get_docstring(node)
                    )
                    file_node.entities.append(entity)
                    self.entities[f"{file_node.file_path}:{node.name}"] = entity
                
                elif isinstance(node, ast.ClassDef):
                    entity = CodeEntity(
                        name=node.name,
                        entity_type='class',
                        file_path=file_node.file_path,
                        line_number=node.lineno,
                        signature=f"class {node.name}(...)",
                        docstring=ast.get_docstring(node)
                    )
                    file_node.entities.append(entity)
                    self.entities[f"{file_node.file_path}:{node.name}"] = entity
                
                elif isinstance(node, ast.Import):
                    for alias in node.names:
                        file_node.imports.append(alias.name)
                
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        file_node.imports.append(node.module)
        
        except SyntaxError as e:
            logger.warning("PythonËØ≠Ê≥ïÈîôËØØ %s: %s", file_node.file_path, e)
    
    def _analyze_js_file(self, file_node: FileNode, content: str):
        """ÂàÜÊûêJavaScript/TypeScriptÊñá‰ª∂ (ÁÆÄÂåñÁâà)"""
        # ÁÆÄÂåñÁöÑÊ≠£ÂàôË°®ËææÂºèÂàÜÊûê
        
        # Êü•ÊâæÂáΩÊï∞ÂÆö‰πâ
        function_patterns = [
            r'function\s+(\w+)\s*\(',
            r'const\s+(\w+)\s*=\s*\(',
            r'(\w+)\s*:\s*function\s*\(',
            r'(\w+)\s*=>\s*'
        ]
        
        for pattern in function_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                entity = CodeEntity(
                    name=match.group(1),
                    entity_type='function',
                    file_path=file_node.file_path,
                    line_number=line_num,
                    signature=match.group(0)
                )
                file_node.entities.append(entity)
                self.entities[f"{file_node.file_path}:{match.group(1)}"] = entity
        
        # Êü•ÊâæÁ±ªÂÆö‰πâ
        class_pattern = r'class\s+(\w+)'
        matches = re.finditer(class_pattern, content)
        for match in matches:
            line_num = content[:match.start()].count('\n') + 1
            entity = CodeEntity(
                name=match.group(1),
                entity_type='class',
                file_path=file_node.file_path,
                line_number=line_num,
                signature=match.group(0)
            )
            file_node.entities.append(entity)
            self.entities[f"{file_node.file_path}:{match.group(1)}"] = entity
        
        # Êü•ÊâæimportËØ≠Âè•
        import_patterns = [
            r'import\s+.*\s+from\s+["\']([^"\']+)["\']',
            r'import\s+["\']([^"\']+)["\']',
            r'require\(["\']([^"\']+)["\']\)'
        ]
        
        for pattern in import_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                file_node.imports.append(match.group(1))
    
    def _build_dependency_graph(self) -> int:
        """ÊûÑÂª∫‰æùËµñÂÖ≥Á≥ªÂõæ"""
        dependency_count = 0
        
        for file_path, file_node in self.files.items():
            # Ê∑ªÂä†Êñá‰ª∂ËäÇÁÇπÂà∞Âõæ‰∏≠
            self.graph.add_node(file_path, node_type='file', data=file_node)
            
            # Ê∑ªÂä†import‰æùËµñ
            for import_name in file_node.imports:
                # Â∞ùËØïËß£ÊûêÂà∞ÂÆûÈôÖÊñá‰ª∂
                target_file = self._resolve_import(import_name, file_path)
                if target_file and target_file in self.files:
                    self.graph.add_edge(file_path, target_file, 
                                       relation_type='imports', 
                                       strength=0.8)
                    dependency_count += 1
            
            # Ê∑ªÂä†ÂÆû‰ΩìËäÇÁÇπÂíåÂÖ≥Á≥ª
            for entity in file_node.entities:
                entity_key = f"{file_path}:{entity.name}"
                self.graph.add_node(entity_key, node_type='entity', data=entity)
                self.graph.add_edge(file_path, entity_key, 
                                   relation_type='contains',
                                   strength=1.0)
        
        return dependency_count
    
    def _resolve_import(self, import_name: str, from_file: str) -> Optional[str]:
        """Ëß£ÊûêimportÂà∞ÂÆûÈôÖÊñá‰ª∂Ë∑ØÂæÑ"""
        # ÁÆÄÂåñÁâàÊú¨ÁöÑimportËß£Êûê
        if import_name.startswith('.'):
            # Áõ∏ÂØπÂØºÂÖ•
            from_dir = Path(from_file).parent
            if import_name.startswith('./'):
                import_path = from_dir / import_name[2:]
            elif import_name.startswith('../'):
                import_path = from_dir / import_name
            else:
                import_path = from_dir / import_name[1:]
            
            # Â∞ùËØï‰∏çÂêåÁöÑÊâ©Â±ïÂêç
            for ext in self.supported_extensions:
                potential_file = import_path.with_suffix(ext)
                if potential_file.exists():
                    return str(potential_file)
        
        return None
    
    def _analyze_entity_relationships(self) -> int:
        """ÂàÜÊûêÂÆû‰ΩìÂÖ≥Á≥ª"""
        return len(self.entities)
    
    def _calculate_risk_scores(self):
        """ËÆ°ÁÆóÈ£éÈô©ËØÑÂàÜ"""
        for file_path, file_node in self.files.items():
            # Âü∫‰∫éÂ§çÊùÇÂ∫¶Âíå‰æùËµñÂÖ≥Á≥ªËÆ°ÁÆóÈ£éÈô©
            risk_factors = []
            
            # Êñá‰ª∂Â§ßÂ∞èÈ£éÈô©
            if file_node.size_bytes > 10000:  # >10KB
                risk_factors.append(0.3)
            
            # Ë°åÊï∞È£éÈô©
            if file_node.line_count > 500:
                risk_factors.append(0.4)
            
            # ÂÆû‰ΩìÊï∞ÈáèÈ£éÈô©
            if len(file_node.entities) > 20:
                risk_factors.append(0.3)
            
            # ‰æùËµñÊï∞ÈáèÈ£éÈô©
            if len(file_node.imports) > 15:
                risk_factors.append(0.2)
            
            file_node.risk_score = min(1.0, sum(risk_factors))
    
    def _update_context_statistics(self):
        """Êõ¥Êñ∞‰∏ä‰∏ãÊñáÁªüËÆ°‰ø°ÊÅØ"""
        if self.context:
            self.context.total_files = len(self.files)
            self.context.total_lines = sum(f.line_count for f in self.files.values())
            
            # Â§çÊùÇÂ∫¶ÂàÜÂ∏É
            complexity_levels = {'low': 0, 'medium': 0, 'high': 0, 'extreme': 0}
            for file_node in self.files.values():
                if file_node.risk_score < 0.3:
                    complexity_levels['low'] += 1
                elif file_node.risk_score < 0.6:
                    complexity_levels['medium'] += 1
                elif file_node.risk_score < 0.8:
                    complexity_levels['high'] += 1
                else:
                    complexity_levels['extreme'] += 1
            
            self.context.complexity_distribution = complexity_levels
    
    def get_entity_by_name(self, name: str) -> List[CodeEntity]:
        """Ê†πÊçÆÂêçÁß∞Êü•ÊâæÂÆû‰Ωì"""
        return [entity for entity in self.entities.values() if name in entity.name]
    
    def get_file_dependencies(self, file_path: str) -> List[str]:
        """Ëé∑ÂèñÊñá‰ª∂‰æùËµñ"""
        if file_path in self.graph:
            return list(self.graph.successors(file_path))
        return []
    
    def get_file_dependents(self, file_path: str) -> List[str]:
        """Ëé∑Âèñ‰æùËµñÊ≠§Êñá‰ª∂ÁöÑÂÖ∂‰ªñÊñá‰ª∂"""
        if file_path in self.graph:
            return list(self.graph.predecessors(file_path))
        return []
    
    def predict_change_impact(self, file_path: str) -> Dict[str, Any]:
        """È¢ÑÊµã‰ª£Á†ÅÂèòÊõ¥ÂΩ±Âìç"""
        if file_path not in self.graph:
            return {'error': 'File not found in graph'}
        
        # Áõ¥Êé•‰æùËµñ
        direct_dependents = self.get_file_dependents(file_path)
        
        # Èó¥Êé•‰æùËµñ (2Â∫¶‰ª•ÂÜÖ)
        indirect_dependents = set()
        for dep in direct_dependents:
            indirect_dependents.update(self.get_file_dependents(dep))
        
        # È£éÈô©ËØÑ‰º∞
        risk_level = 'low'
        impact_files = len(direct_dependents) + len(indirect_dependents)
        
        if impact_files > 20:
            risk_level = 'extreme'
        elif impact_files > 10:
            risk_level = 'high'
        elif impact_files > 5:
            risk_level = 'medium'
        
        return {
            'target_file': file_path,
            'direct_impact': direct_dependents,
            'indirect_impact': list(indirect_dependents),
            'total_impact_files': impact_files,
            'risk_level': risk_level,
            'recommendations': self._get_change_recommendations(risk_level, impact_files)
        }
    
    def _get_change_recommendations(self, risk_level: str, impact_files: int) -> List[str]:
        """Ëé∑ÂèñÂèòÊõ¥Âª∫ËÆÆ"""
        recommendations = []
        
        if risk_level == 'extreme':
            recommendations.extend([
                "üö® È´òÈ£éÈô©ÂèòÊõ¥ÔºöÂΩ±ÂìçË∂ÖËøá20‰∏™Êñá‰ª∂",
                "Âª∫ËÆÆÂàÜÈò∂ÊÆµÂÆûÊñΩÂèòÊõ¥",
                "ÂøÖÈ°ªËøõË°åÂÖ®Èù¢ÊµãËØï",
                "ËÄÉËôëÂäüËÉΩÂºÄÂÖ≥ÊéßÂà∂ÂèëÂ∏É"
            ])
        elif risk_level == 'high':
            recommendations.extend([
                "‚ö†Ô∏è  ‰∏≠È´òÈ£éÈô©ÂèòÊõ¥ÔºöÂΩ±Âìç10-20‰∏™Êñá‰ª∂",
                "Âª∫ËÆÆÂ¢ûÂä†ÈõÜÊàêÊµãËØï",
                "ÈÄöÁü•Áõ∏ÂÖ≥Âõ¢ÈòüÊàêÂëò"
            ])
        elif risk_level == 'medium':
            recommendations.extend([
                "üìã ‰∏≠Á≠âÈ£éÈô©ÂèòÊõ¥ÔºöÂΩ±Âìç5-10‰∏™Êñá‰ª∂",
                "Âª∫ËÆÆËøõË°åÂõûÂΩíÊµãËØï"
            ])
        else:
            recommendations.append("‚úÖ ‰ΩéÈ£éÈô©ÂèòÊõ¥ÔºöÂΩ±ÂìçËæÉÂ∞è")
        
        return recommendations
    
    def export_project_summary(self) -> Dict[str, Any]:
        """ÂØºÂá∫È°πÁõÆÊëòË¶Å"""
        if not self.context:
            return {'error': 'Project not analyzed'}
        
        # Ëé∑ÂèñÂÖ≥ÈîÆÁªüËÆ°‰ø°ÊÅØ
        high_risk_files = [f for f, node in self.files.items() if node.risk_score > 0.7]
        
        # ÊúÄÂ§çÊùÇÁöÑÊñá‰ª∂
        complex_files = sorted(self.files.items(), 
                              key=lambda x: x[1].risk_score, 
                              reverse=True)[:10]
        
        # Ê†∏ÂøÉÂÆû‰Ωì
        core_entities = [e for e in self.entities.values() if e.usage_count > 5]
        
        return {
            'project_context': asdict(self.context),
            'statistics': {
                'total_files': len(self.files),
                'total_entities': len(self.entities),
                'dependency_relationships': self.graph.number_of_edges(),
                'high_risk_files': len(high_risk_files),
                'complexity_distribution': self.context.complexity_distribution
            },
            'high_risk_files': [{'path': f, 'risk_score': self.files[f].risk_score} 
                               for f in high_risk_files[:10]],
            'most_complex_files': [{'path': f[0], 'risk_score': f[1].risk_score, 'lines': f[1].line_count} 
                                  for f in complex_files],
            'core_entities': [{'name': e.name, 'type': e.entity_type, 'file': e.file_path} 
                             for e in core_entities[:20]]
        }
